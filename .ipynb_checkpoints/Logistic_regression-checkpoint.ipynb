{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression\n",
    "[Logistic regression](https://en.wikipedia.org/wiki/Logistic_regression) is similar to `linear regression`, but instead of predicting a continuous output, classifies training examples by a set of categories or labels.  For example, linear regression on a set of social and economic data might be used to predict a person's income, but logistic regression could be used to predict whether that person was married, had children, or had ever been arrested.  In a basic sense, logistic regression only answers questions that have yes / no answers, or questions that can be answered with a 1 or 0.  However, it can easily be [extended](https://en.wikipedia.org/wiki/Multinomial_logistic_regression) to problems where there are a larger set of categories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizate the sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Define the sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Generate a range of x values\n",
    "x_range = np.linspace(-10, 10, 500)  # Covers a wide range to show the curve\n",
    "sigmoid_values = sigmoid(x_range)\n",
    "\n",
    "# Plot the sigmoid curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(x_range, sigmoid_values, label=\"Sigmoid Curve\", color=\"red\", linewidth=2)\n",
    "\n",
    "# Adding labels and title\n",
    "# plt.axvline(0, color='gray', linestyle='--', alpha=0.7)  # Highlight x=0 (decision boundary)\n",
    "# plt.axhline(0.5, color='gray', linestyle='--', alpha=0.7)  # Highlight y=0.5\n",
    "plt.xlabel(\"Input Values (x)\")\n",
    "plt.ylabel(\"Sigmoid Output (S(x))\")\n",
    "plt.title(\"<Your name> + Sigmoid Function Curve\")\n",
    "plt.grid(alpha=0.4)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:blue\" align=\"center\">Predicting if a person would buy life insurnace based on age using logistic regression</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here is a binary [logistic regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) problem as there are only two possible outcomes (i.e. if person buys insurance or doesn't). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# read csv by pandas\n",
    "df = pd.read_csv(\"insurance_data.csv\")\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the distribution of insurance\n",
    "# we use the scatter chart in here\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(df.age,df.bought_insurance,marker='+',color='red')\n",
    "plt.ylabel('Have_insurance')\n",
    "plt.xlabel('Age')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# select age as X\n",
    "X = df[['age']].values\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select boolean value -- bought_insurance as Y\n",
    "y=df.bought_insurance.values\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create Logistic regression object\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input value is age, \n",
    "# The trained model will give us the corresponding boolean value -- bought insurance or not.\n",
    "\n",
    "y_predicted = model.predict(X_test)\n",
    "y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**model.coef_ indicates value of m in y=m*x + b equation**\n",
    "\n",
    "In logistic regression, we can consider y = m*f(x)+b, where f(x) is the sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**model.intercept_ indicates value of b in y=m*x + b equation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lets defined sigmoid function now and do the math with hand**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a fairly obvious area near the center of the plot where a line could divide the two colors of points with a small amount of error.\n",
    "\n",
    "# Simple Logistic Regression\n",
    "\n",
    "To implement logistic regression, I need a hypothesis function $h_\\theta(x)$, a cost function $J(\\theta)$, and a gradient function that computes the partial derivatives of $J(\\theta)$.\n",
    "\n",
    "In logistic regression, $h_\\theta$ is the [sigmoid](https://www.quora.com/Logistic-Regression-Why-sigmoid-function) function.  The sigmoid function is bounded between 0 and 1, and produces a value that can be interpreted as a probability.  This value can also be a yes / no answer with a cross-over, or decision boundary, at 0.5:\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = \\frac{1}{1 + e^{ \\theta^Tx}}\n",
    "$$\n",
    "\n",
    "Using mean squared error from linear regression isn't a good idea here, as the resulting cost function [isn't convex](http://mathworld.wolfram.com/SigmoidFunction.html) and so is not well-suited for gradient descent.  Instead, the difference of $h_\\theta(x^i) - y^i$ is calculated differently for $y=0$ and $y=1$, and the result is [transformed](https://stackoverflow.com/questions/32986123/why-the-cost-function-of-logistic-regression-has-a-logarithmic-expression) [logarithmically](https://math.stackexchange.com/questions/886555/deriving-cost-function-using-mle-why-use-log-function) into a convex function.\n",
    "\n",
    "[Log loss (Cross-entropy)](https://machinelearningmastery.com/cross-entropy-for-machine-learning/) function:\n",
    "$$\n",
    "J(\\theta) =-\\frac{1}{m}\\sum_{i=1}^{m}y^{i}\\log(h_\\theta(x^{i}))+(1-y^{i})\\log(1-h_\\theta(x^{i}))\n",
    "$$\n",
    "\n",
    "Fortunately, the [derivative](https://math.stackexchange.com/questions/477207/derivative-of-cost-function-for-logistic-regression) of this function is exactly the same as that of linear regression, just with a different $h_\\theta(x)$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial\\theta_{j}}J(\\theta) =\\sum_{i=1}^{m}(h_\\theta(x^{i})-y^i)x_j^i\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(X):\n",
    "    return 1 / (1 + np.exp(-X))\n",
    "\n",
    "# Define the cost function for logistic regression\n",
    "def cost_function(theta, X, y):\n",
    "    \"\"\"\n",
    "    Computes the cost function for logistic regression.\n",
    "    \n",
    "    Parameters:\n",
    "    theta: ndarray\n",
    "        Model parameters (weights), a 1D array.\n",
    "    X: ndarray\n",
    "        Feature matrix where rows are samples and columns are features.\n",
    "    y: ndarray\n",
    "        Target values, a 1D array of binary labels (0 or 1).\n",
    "\n",
    "    Returns:\n",
    "    float\n",
    "        The cost value, a measure of how well the model predicts the target values.\n",
    "    \"\"\"\n",
    "\n",
    "    # Number of training examples\n",
    "    m = len(y)\n",
    "\n",
    "    # Compute the predicted probabilities using the sigmoid function\n",
    "    # h represents the model's predictions for each training example\n",
    "    h = sigmoid(X.dot(theta))\n",
    "\n",
    "    # Compute the cost function for logistic regression\n",
    "    # - y.dot(np.log(h)) computes the log loss for correctly predicted examples\n",
    "    # - (1 - y).dot(np.log(1 - h)) computes the log loss for incorrectly predicted examples\n",
    "    # -(1 / m) scales the total loss by the number of training examples to compute the average\n",
    "    cost = -(1 / m) * (y.dot(np.log(h)) + (1 - y).dot(np.log(1 - h)))\n",
    "\n",
    "    return cost\n",
    "\n",
    "\n",
    "# Define the gradient function for logistic regression\n",
    "def gradient(theta, X, y):\n",
    "    \"\"\"\n",
    "    Computes the gradient of the cost function with respect to the model parameters.\n",
    "    \n",
    "    Parameters:\n",
    "    theta: ndarray\n",
    "        Model parameters (weights), a 1D array.\n",
    "    X: ndarray\n",
    "        Feature matrix where rows are samples and columns are features.\n",
    "    y: ndarray\n",
    "        Target values, a 1D array of binary labels (0 or 1).\n",
    "\n",
    "    Returns:\n",
    "    ndarray\n",
    "        Gradient vector, indicating the direction and magnitude of change for each parameter.\n",
    "    \"\"\"\n",
    "    \n",
    "    m = len(y)  # Number of training examples\n",
    "    h = sigmoid(X.dot(theta))  # Predicted probabilities using the sigmoid function\n",
    "    gradient = (1 / m) * X.T.dot(h - y)  # Compute the gradient vector\n",
    "\n",
    "    return gradient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's worth noting the treatment of `y` and `theta` above.  In each function, we explicitly convert each to an $n$ or $m \\times 1$ `ndarray`, so the matrix operations work correctly.  An alternative is to use a numpy `matrix`, which has stricter linear algebra semantics and treats 1-dimensional matrices more like column vectors.  However, we found that it was awkward to get the matrix interface to work correctly with both the optimization function used below, and with TensorFlow.  The indexing syntax can be thought of as explicitly columnizing the array of parameters or labels.\n",
    "\n",
    "Instead of manually writing a gradient descent, we use an optimization algorithm from Scipy called [`minimize`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html) to perform it.  This function takes as parameters the cost function, an initial set of parameters for $\\theta$, the gradient function, and a tuple of args to pass to each.  We define a `train` function that prepends a columns of 1s to the training data (allowing for a bias parameter $\\theta_0$), run the minimization function and return the first of its return values, final parameters for $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def train(X, y):\n",
    "\n",
    "    # Reshape X to include a bias term (intercept)\n",
    "    X = np.column_stack((np.ones(len(X)), X))  # Adds a column of 1s for the intercept\n",
    "    \n",
    "    # Initialize theta (parameters) as a zero vector with the same number of features as X\n",
    "    theta = np.zeros(X.shape[1])\n",
    "    \n",
    "    # Minimize the cost function using scipy.optimize.minimize\n",
    "    # - fun: The cost function to minimize\n",
    "    # - x0: Initial guess for theta (parameters)\n",
    "    # - args: Additional arguments passed to the cost function (X and y)\n",
    "    # - method: Optimization algorithm (\"TNC\" in this case)\n",
    "    # - jac: The gradient function used for optimization\n",
    "    result = minimize(fun=cost_function, x0=theta, args=(X, y), method='TNC', jac=gradient, options = {'maxfun': 4000})\n",
    "    \n",
    "    \n",
    "    return result\n",
    "\n",
    "theta = train(X_train, y_train)\n",
    "\n",
    "# Extract model coefficients\n",
    "theta_optimized = theta.x\n",
    "intercept = theta_optimized[0]  # Intercept\n",
    "coef = theta_optimized[1:]      # Coefficients\n",
    "\n",
    "print('theta: ', intercept, coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-5.83209658 + 0.15639179 * x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict function\n",
    "def predict(X, theta):\n",
    "    probabilities = sigmoid(X.dot(theta))\n",
    "    return (probabilities >= 0.5).astype(int)\n",
    "\n",
    "# Make predictions\n",
    "X_test_C = np.column_stack((np.ones(len(X_test)), X_test))\n",
    "# Make predictions on the test set\n",
    "predictions_test = predict(X_test_C, theta_optimized)\n",
    "\n",
    "# Calculate accuracy on the test set\n",
    "accuracy_test = np.mean(predictions_test == y_test) * 100\n",
    "print(\"Test Accuracy:\", accuracy_test, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the logistic regression fit\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Scatter plot of actual data\n",
    "plt.scatter(X, y, color='blue', label='Actual Data')\n",
    "\n",
    "# Logistic regression curve\n",
    "age_range = np.linspace(X.min(), X.max(), 300).reshape(-1, 1)\n",
    "\n",
    "X_range = np.column_stack((np.ones(len(age_range)), age_range)) # \n",
    "\n",
    "probabilities = sigmoid(X_range.dot(theta_optimized)) #\n",
    "\n",
    "plt.plot(age_range, probabilities, color='red', label='Logistic Regression Curve')\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Probability of Buying Insurance')\n",
    "plt.title('<Your name> +Logistic Regression: Age vs Bought Insurance')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
